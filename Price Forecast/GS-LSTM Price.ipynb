{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BTC_Trial_NB.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "df = df.drop(['Date', 'Month', 'Year', 'Network Difficulty'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "df_for_training=df[:1848]\n",
    "df_for_testing=df[1848:]\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "df_for_training_scaled = scaler.fit_transform(df_for_training)\n",
    "\n",
    "df_for_testing_scaled=scaler.transform(df_for_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_training_scaled.shape, df_for_testing_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create the reshaped input vector for the LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createXY(dataset,n_past):\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for i in range(n_past, len(dataset)):\n",
    "            dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "            dataY.append(dataset[i,3])\n",
    "    return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of the model is the scaled data of the closing price, with the time atep (window) of 30 observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,trainY=createXY(df_for_training_scaled,30)\n",
    "testX,testY=createXY(df_for_testing_scaled,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Deep learning modelling: LSTM model with 1 LSTM layer and 2 Dense layers (Vanilla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers  import *\n",
    "from tensorflow.keras.losses import MeanSquaredError \n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer((trainX.shape[1], trainX.shape[2])))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(8, 'relu'))\n",
    "model.add(Dense(1, 'linear'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = MeanSquaredError(), optimizer= Adam(learning_rate= 0.001), metrics= [RootMeanSquaredError()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), epochs=20, batch_size=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Model's prediction and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model's prediction on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model.predict(trainX).flatten()\n",
    "train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = pd.DataFrame(data= {'Train Pred':train_pred, 'Actual':trainY.flatten()})\n",
    "train_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_result['Actual'])\n",
    "plt.plot(train_result['Train Pred'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model's prediction on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(testX).flatten()\n",
    "test_result = pd.DataFrame(data= {'Test Pred':test_pred, 'Actual':testY.flatten()})\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_result['Actual'])\n",
    "plt.plot(test_result['Test Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMSE of the predicted value on the Test data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_1 = np.sqrt(mean_squared_error(testY, test_pred))\n",
    "RMSE_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter tuning: Tuning model with 2 lstm layers and 2 Dense layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialize the model tunning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer((trainX.shape[1], trainX.shape[2])))\n",
    "    \n",
    "    lstm_units = hp.Choice('lstm_units', values=[32, 64, 128, 256])\n",
    "    model.add(LSTM(lstm_units, return_sequences=True))\n",
    "    \n",
    "    dropout_rate = hp.Choice('dropout_rate', values=[0.05, 0.1, 0.15, 0.2])\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(LSTM(lstm_units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    activation_function = hp.Choice('activation_function', values=['relu', 'tanh'])\n",
    "    dense_units = hp.Choice('dense_units', values=[8, 16, 32, 64])  \n",
    "    model.add(Dense(dense_units, activation=activation_function))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    \n",
    "    learning_rate = hp.Choice('learning_rate', values=[0.001, 0.01, 0.1])\n",
    "    model.compile(\n",
    "        loss=MeanSquaredError(),\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.GridSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    executions_per_trial=1,\n",
    "    directory='my_model10',\n",
    "    project_name='lstm_tuning'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Start the tuning process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the number of epoch equals 20 with the batch size of 32. The model uses the Test set as the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search(\n",
    "    trainX, trainY,\n",
    "    epochs= 100,\n",
    "    batch_size= 64,\n",
    "    validation_data=(testX, testY)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtaining the best performing model and set it to \"best_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the result of the 10 best performing model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the hyperparameters from the best performing model and set it to \"best_hp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"\"\"\n",
    "The best hyperparameters are:\n",
    "- lstm_units: {best_hps.get('lstm_units')}\n",
    "- dropout_rate: {best_hps.get('dropout_rate')}\n",
    "- activation function: {best_hps.get('activation_function')}\n",
    "- dense_units: {best_hps.get('dense_units')}\n",
    "- learning_rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting the value of the Test data using best_model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(testX)\n",
    "# best_test_result = pd.DataFrame(data= {'Test Pred':y_test_best_pred, 'Actual':testY.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_copies_array = np.repeat(y_pred,23, axis=-1)\n",
    "original_copies_array = np.repeat(testY,23, axis=-1)\n",
    "\n",
    "pred = scaler.inverse_transform(np.reshape(prediction_copies_array,(len(y_pred),23)))[:,3]\n",
    "original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),23)))[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening the predictions and actual values\n",
    "y_test_flat = np.ravel(original)\n",
    "y_pred_flat = np.ravel(pred)\n",
    "\n",
    "# Plotting\n",
    "indices = np.arange(len(y_test_flat))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(indices, y_test_flat, label='Actual Price', color='blue')\n",
    "plt.plot(indices, y_pred_flat, label='Predicted Price', color='red', linestyle='--')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Actual vs. Predicted Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# MAPE\n",
    "def mape(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "#NMSE\n",
    "def nmse(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    variance = np.var(y_true)\n",
    "    return mse / variance\n",
    "\n",
    "#DA\n",
    "def DA(y_true, y_pred):\n",
    "    # Convert the arrays to numpy arrays\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calculate the direction of change\n",
    "    true_direction = np.sign(np.diff(y_true))\n",
    "    pred_direction = np.sign(np.diff(y_pred))\n",
    "    \n",
    "    # Compare directions\n",
    "    correct_direction = np.sum(true_direction == pred_direction)\n",
    "    total_direction = len(true_direction)\n",
    "    \n",
    "    # Calculate directional accuracy\n",
    "    da = correct_direction / total_direction * 100\n",
    "    \n",
    "    return da\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = y_test_flat\n",
    "y_predi = y_pred_flat\n",
    "\n",
    "print(\"RMSE: \", rmse(y_true, y_predi))\n",
    "print(\"MAPE: \", mape(y_true, y_predi))\n",
    "print(\"NMSE: \", nmse(y_true, y_predi))\n",
    "print(\"DA: \", DA(y_true, y_predi))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
